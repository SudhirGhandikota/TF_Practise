{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow_graph_in_jupyter import show_graph\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crteating basic computational graph\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# opening and running a tensorflow session - initialize the variables and evaluate\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run() #initializing variable x\n",
    "    y.initializer.run() #initializing variable y\n",
    "    result = f.eval() #evaluating f\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a global_variable_initializer() to initialize all variables\n",
    "# it does not perform the initialization immediately. It creates a node in the graph that will initialize all variables when it is run\n",
    "init = tf.global_variables_initializer() #prepare an init node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run() #actually initialize all variables\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### important points #########################\n",
    "# 1. When we evaluate a node, TensorFlow automatically determines the set of nofes that it depends on and evaluates them first\n",
    "# 2. All node values are dropped between graph runs, except variable values, which are maintained by the session across graph runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# evaluating nodes in two different runs. All intermediate nodes are recomputed between runs\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x + 3\n",
    "# both evaluations done in separate runs\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) #10\n",
    "    print(z.eval()) #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# evaluating nodes in the same run. All intermediate nodes are shared\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x + 3\n",
    "# both evaluations done in same run\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val) #10\n",
    "    print(z_val) #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Linear Regression on TensorFlow ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n"
     ]
    }
   ],
   "source": [
    "# fetching the california housing dataset from sklearn\n",
    "housing = fetch_california_housing()\n",
    "print(housing.data.shape)\n",
    "m, n = housing.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "# adding the bias input feature\n",
    "housing_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "print(housing_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tensor flow nodes for the data observations and labels\n",
    "# X->data tensor, a constant\n",
    "X = tf.constant(housing_bias, dtype = tf.float32, name = \"X\")\n",
    "# converting the labels from a 1D array to a column vector\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name = \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposing data matrix\n",
    "Xt = tf.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating theta hat (estimators) of the linear regression problem -> normal equation\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(Xt, X)),Xt),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating theta\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.6959320e+01]\n",
      " [ 4.3698898e-01]\n",
      " [ 9.4245886e-03]\n",
      " [-1.0791138e-01]\n",
      " [ 6.4842808e-01]\n",
      " [-3.9986235e-06]\n",
      " [-3.7866351e-03]\n",
      " [-4.2142656e-01]\n",
      " [-4.3467718e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Linear Regression with pure numpy ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# X -> data matrix\n",
    "X = housing_bias\n",
    "# y -> labels\n",
    "y = housing.target.reshape(-1, 1)\n",
    "# computing theta using numpy linear algebra package\n",
    "# X.T -> X transpose\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Linear Regression with scikit-learn ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 0.00000000e+00]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "#initializing linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "#fitting linear regression model for the data in hand\n",
    "lin_reg.fit(X, y)\n",
    "#theta = intercept + coefficients\n",
    "# np.r_ -> concatenate horizontally, \n",
    "theta_scikit = np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T]\n",
    "print(theta_scikit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Linear Regression using gradient descent - manual ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling the data to make sure gradient descent converges\n",
    "scaler = StandardScaler()\n",
    "# adding the bias term after scaling\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_bias = np.c_[ np.ones((m,1)), scaled_housing_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_epochs = 1000 #number of epochs\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tensor flow nodes for the data observations and labels\n",
    "# X->data tensor, a constant\n",
    "X = tf.constant(scaled_housing_bias, dtype = tf.float32, name = \"X\")\n",
    "# converting the labels from a 1D array to a column vector\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name = \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_function() creates a tensor (vector) containing randomly generated values\n",
    "# dimension of theta = number of features + bias = 9 * 1\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0 , 1.0), name= \"theta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to hold the predicted regression values\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\") # y_pred = X.theta, dimension = 20640 * 1\n",
    "# initializing error term: predicted_values - actual_values\n",
    "error = y_pred - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to hold mean squared error\n",
    "# reduce_mean -> computes mean of elements along the dimension\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to hold gradients, calculated as 2/m * XT.[y_pred - y]\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error) # dimension: 9 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch gradient descent: theta = theta - learning_rata * gradients. Dimension: 9 * 1\n",
    "# assign() creates a node that will assign a new value to a variable\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global initializer\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE: 12.408014\n",
      "Epoch:  100 MSE: 0.75519687\n",
      "Epoch:  200 MSE: 0.5420874\n",
      "Epoch:  300 MSE: 0.5331699\n",
      "Epoch:  400 MSE: 0.53053814\n",
      "Epoch:  500 MSE: 0.5287963\n",
      "Epoch:  600 MSE: 0.527549\n",
      "Epoch:  700 MSE: 0.52664965\n",
      "Epoch:  800 MSE: 0.5260011\n",
      "Epoch:  900 MSE: 0.52553326\n"
     ]
    }
   ],
   "source": [
    "# executing the session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #initializing all the global variables\n",
    "    # running the training over 1000 epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # printing the MSE for every 100th epoch\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Epoch: \", epoch, \"MSE:\", mse.eval())\n",
    "        #computing the gradient\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Linear Regression using gradient descent - autodiff ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tensor flow nodes for the data observations and labels\n",
    "X = tf.constant(scaled_housing_bias, dtype = tf.float32, name = \"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name = \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing theta, y_pred, error and mse same as above\n",
    "theta = tf.Variable(tf.random_uniform([n+1 , 1], -1, 1, seed = 42), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of calculating gradients manually using autodiff() through gradients()\n",
    "# It takes an op(mse) and a list of variables(theta) and creates a list of ops one for each variable to compute gradients of op with each variable\n",
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a training operation\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "# global initializer\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE: 9.161542\n",
      "Epoch:  100 MSE: 0.7145004\n",
      "Epoch:  200 MSE: 0.56670487\n",
      "Epoch:  300 MSE: 0.55557173\n",
      "Epoch:  400 MSE: 0.5488112\n",
      "Epoch:  500 MSE: 0.5436363\n",
      "Epoch:  600 MSE: 0.53962904\n",
      "Epoch:  700 MSE: 0.5365092\n",
      "Epoch:  800 MSE: 0.53406775\n",
      "Epoch:  900 MSE: 0.5321473\n"
     ]
    }
   ],
   "source": [
    "# executing the session - same as above\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #initializing all the global variables\n",
    "    # running the training over 1000 epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # printing the MSE for every 100th epoch\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Epoch: \", epoch, \"MSE:\", mse.eval())\n",
    "        #computing the gradient\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Linear Regression using gradient descent optimizer ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializations same as above\n",
    "# creating tensor flow nodes for the data observations and labels\n",
    "X = tf.constant(scaled_housing_bias, dtype = tf.float32, name = \"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name = \"y\")\n",
    "# initializing theta, y_pred, error and mse same as above\n",
    "theta = tf.Variable(tf.random_uniform([n+1 , 1], -1, 1, seed = 42), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tensorflow provided pre-defined optimizers (StochasticGradientDescent in this case)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instead of explicitly stating the training operation, optimizer object can be used\n",
    "# Objective: minimizing mse\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE: 9.161542\n",
      "Epoch:  100 MSE: 0.7145004\n",
      "Epoch:  200 MSE: 0.56670487\n",
      "Epoch:  300 MSE: 0.55557173\n",
      "Epoch:  400 MSE: 0.5488112\n",
      "Epoch:  500 MSE: 0.5436363\n",
      "Epoch:  600 MSE: 0.53962904\n",
      "Epoch:  700 MSE: 0.5365092\n",
      "Epoch:  800 MSE: 0.53406775\n",
      "Epoch:  900 MSE: 0.5321473\n",
      "[[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.6614528 ]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "# executing the session - same as above\n",
    "# global initializer\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #initializing all the global variables\n",
    "    # running the training over 1000 epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # printing the MSE for every 100th epoch\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Epoch: \", epoch, \"MSE:\", mse.eval())\n",
    "        #computing the gradient\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Linear Regression using momentum optimizer ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializations same as above\n",
    "# creating tensor flow nodes for the data observations and labels\n",
    "X = tf.constant(scaled_housing_bias, dtype = tf.float32, name = \"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name = \"y\")\n",
    "# initializing theta, y_pred, error and mse same as above\n",
    "theta = tf.Variable(tf.random_uniform([n+1 , 1], -1, 1, seed = 42), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tensorflow provided pre-defined optimizers (StochasticGradientDescent in this case)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instead of explicitly stating the training operation, optimizer object can be used\n",
    "# Objective: minimizing mse\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE: 9.161542\n",
      "Epoch:  100 MSE: 0.53056407\n",
      "Epoch:  200 MSE: 0.52501124\n",
      "Epoch:  300 MSE: 0.52441067\n",
      "Epoch:  400 MSE: 0.52433294\n",
      "Epoch:  500 MSE: 0.5243226\n",
      "Epoch:  600 MSE: 0.5243211\n",
      "Epoch:  700 MSE: 0.524321\n",
      "Epoch:  800 MSE: 0.524321\n",
      "Epoch:  900 MSE: 0.524321\n",
      "[[ 2.068558  ]\n",
      " [ 0.8296286 ]\n",
      " [ 0.11875337]\n",
      " [-0.26554456]\n",
      " [ 0.3057109 ]\n",
      " [-0.00450251]\n",
      " [-0.03932662]\n",
      " [-0.89986444]\n",
      " [-0.87052065]]\n"
     ]
    }
   ],
   "source": [
    "# executing the session - same as above\n",
    "# global initializer\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #initializing all the global variables\n",
    "    # running the training over 1000 epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # printing the MSE for every 100th epoch\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Epoch: \", epoch, \"MSE:\", mse.eval())\n",
    "        #computing the gradient\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Linear Regression using min-batch gradient descent ######################\n",
    "# placeholder nodes don't actually perform any computation, they just output the data we tell them to output at runtime\n",
    "# we pass a feed_dict to the eval() method that specify the values of placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function created mini-batches for a given epoch\n",
    "def fetch_data(epoch, batch_index, batch_size):\n",
    "    # randomizing each batch and for every epoch\n",
    "    np.random.seed(epoch * n_batches + batch_size)\n",
    "    # generating random indices equivalent batch_size\n",
    "    indices = np.random.randint(m, size = batch_size)\n",
    "    # filtering observartions and labels to be used for the epoch\n",
    "    X_batch = scaled_housing_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1,1)[indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing (and placing) placeholders for data observations and labels instead of constants\n",
    "# None = \"any size\"\n",
    "X = tf.placeholder(tf.float32, shape = (None, n+1), name = \"X\") # X -> any number of rows, n+1(9) columns\n",
    "y = tf.placeholder(tf.float32, shape = (None, 1), name = \"y\") # y -> any number of rows, 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "n_batches = int(np.ceil(m/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.068558  ]\n",
      " [ 0.82962054]\n",
      " [ 0.11875187]\n",
      " [-0.26552895]\n",
      " [ 0.30569792]\n",
      " [-0.00450293]\n",
      " [-0.03932633]\n",
      " [-0.8998828 ]\n",
      " [-0.8705383 ]]\n"
     ]
    }
   ],
   "source": [
    "# executing the session - same as above\n",
    "# global initializer\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #initializing all the global variables\n",
    "    # running the training over 10 epochs over a batch size of 100\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch ,y_batch = fetch_data(epoch, batch_index, batch_size)\n",
    "            #feeding the values to placeholders X and y\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Saving and restoring models ########################\n",
    "# Saver -> saves and restores all variables under their own name. \n",
    "# By default the saver also saves the graph structure itself in a second file with the extension .meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(scaled_housing_bias, dtype = tf.float32, name = \"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = \"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1, 1, seed = 42), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing Saver instance\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE:  9.161542\n",
      "Epoch:  100 MSE:  0.7145004\n",
      "Epoch:  200 MSE:  0.56670487\n",
      "Epoch:  300 MSE:  0.55557173\n",
      "Epoch:  400 MSE:  0.5488112\n",
      "Epoch:  500 MSE:  0.5436363\n",
      "Epoch:  600 MSE:  0.53962904\n",
      "Epoch:  700 MSE:  0.5365092\n",
      "Epoch:  800 MSE:  0.53406775\n",
      "Epoch:  900 MSE:  0.5321473\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: \",epoch, \"MSE: \", mse.eval())\n",
    "            #saving the model after every 100th epoch\n",
    "            saver.save(sess,\"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "        best_theta = theta.eval()\n",
    "        #saving the final model\n",
    "        saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.6614528 ],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ghai7c/anaconda3/envs/ML_practise/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "#restoring the saved session\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_saved = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if both saved and restores theta values are equal (element-wise)\n",
    "np.allclose(best_theta,best_theta_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "# restoring the graph structure as well. Starting with an empty graph\n",
    "reset_graph()\n",
    "## empty graph\n",
    "# importing meta graph\n",
    "saver = tf.train.import_meta_graph('/tmp/my_model_final.ckpt.meta')\n",
    "#accessing theta op from the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #loading the saved model i.e. restoring the graph's saved state\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.6614528 ],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta_restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the current default graph inside jupyter\n",
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Visualizing graphs using tensorboard ###################\n",
    "# TensorBoard displays interactive visualizations of the provided training stats in a web browser\n",
    "# It reads these training stats from a specified log folder\n",
    "# tensorboard --logdir tf_logs/ (navigate to http://0.0.0.:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "# retrieving the current time stamp\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "# adding timestamp to the name of the log_directory so that each run's stats will be distinct and don't get mixed up\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir  = '{}/run-{}'.format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing model hyperparameters\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing (and placing) placeholders for data observations and labels instead of constants\n",
    "# None = \"any size\"\n",
    "X = tf.placeholder(tf.float32, shape = (None, n+1), name = \"X\") # X -> any number of rows, n+1(9) columns\n",
    "y = tf.placeholder(tf.float32, shape = (None, 1), name = \"y\") # y -> any number of rows, 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing theta, y_pred, error and mse same as above\n",
    "theta = tf.Variable(tf.random_uniform([n+1 , 1], -1, 1, seed = 42), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tensorflow provided pre-defined optimizers (StochasticGradientDescent in this case)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "## instead of explicitly stating the training operation, optimizer object can be used\n",
    "# Objective: minimizing mse\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a node in the graph that will evaluate MSE value and write it to a TensorBoard compatible binary log string called summary\n",
    "mse_summary = tf.summary.scalar('MSE',mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating FileWriter that will be used to write the summaries to logfiles\n",
    "# It also writes the graph definition (of the default graph provided) to a binary logfile called an events file\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the graph execution phase, same as above- using BatchGradientDescent\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_data(epoch, batch_index, batch_size)\n",
    "            # writing the mse summary to a log file after every 10th batch\n",
    "            if batch_index % 10 ==0:\n",
    "                # evaluating the step to compute MSE value over the given batch\n",
    "                summary_str = mse_summary.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            # actual training on the given batch\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1242497 ],\n",
       "       [ 0.90131176],\n",
       "       [ 0.2054878 ],\n",
       "       [-0.1690098 ],\n",
       "       [ 0.48434627],\n",
       "       [-0.11807442],\n",
       "       [-0.4183332 ],\n",
       "       [-0.93492216],\n",
       "       [-0.86516005]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### using Name Scopes to group related nodes ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "# retrieving the current time stamp\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "# adding timestamp to the name of the log_directory so that each run's stats will be distinct and don't get mixed up\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir  = '{}/run-{}'.format(root_logdir, now)\n",
    "\n",
    "# initializing model hyperparameters\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "learning_rate = 0.01\n",
    "\n",
    "# initializing (and placing) placeholders for data observations and labels instead of constants\n",
    "# None = \"any size\"\n",
    "X = tf.placeholder(tf.float32, shape = (None, n+1), name = \"X\") # X -> any number of rows, n+1(9) columns\n",
    "y = tf.placeholder(tf.float32, shape = (None, 1), name = \"y\") # y -> any number of rows, 1 column\n",
    "\n",
    "# initializing theta, y_pred, error and mse same as above\n",
    "theta = tf.Variable(tf.random_uniform([n+1 , 1], -1, 1, seed = 42), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning name scope of \"loss\" to related nodes. Name of each op defined within the scope is prefixed with \"loss/\"\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tensorflow provided pre-defined optimizers (StochasticGradientDescent in this case)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "## instead of explicitly stating the training operation, optimizer object can be used\n",
    "# Objective: minimizing mse\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# creating a node in the graph that will evaluate MSE value and write it to a TensorBoard compatible binary log string called summary\n",
    "mse_summary = tf.summary.scalar('MSE',mse)\n",
    "\n",
    "# creating FileWriter that will be used to write the summaries to logfiles\n",
    "# It also writes the graph definition (of the default graph provided) to a binary logfile called an events file\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.1242497 ]\n",
      " [ 0.90131176]\n",
      " [ 0.2054878 ]\n",
      " [-0.1690098 ]\n",
      " [ 0.48434627]\n",
      " [-0.11807442]\n",
      " [-0.4183332 ]\n",
      " [-0.93492216]\n",
      " [-0.86516005]]\n"
     ]
    }
   ],
   "source": [
    "# running the graph execution phase, same as above- using BatchGradientDescent\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_data(epoch, batch_index, batch_size)\n",
    "            # writing the mse summary to a log file after every 10th batch\n",
    "            if batch_index % 10 ==0:\n",
    "                # evaluating the step to compute MSE value over the given batch\n",
    "                summary_str = mse_summary.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            # actual training on the given batch\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()\n",
    "file_writer.close()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name, mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Illustrating the modularity supported through tensorflow ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function computes a ReLU operation on the given data X\n",
    "def relu(X):\n",
    "    # assigning name score to each node/op\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]),1)\n",
    "        #intiating weights randomly\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name = \"weights\")\n",
    "        #bias term\n",
    "        b = tf.Variable(0.0, name = \"bias\")\n",
    "        # computing a linear function of inputs\n",
    "        z = tf.add(tf.matmul(X, w),b, name = \"z\")\n",
    "        return tf.maximum(z, 0., name = \"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note that the session is not getting executed, we are just visualizing the graph structure\n",
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_features), name = \"X\")\n",
    "#adding 5 relus\n",
    "relus = [relu(X) for i in range(5)]\n",
    "# add_n() creates an operation that will compute the sum of a list of tensors\n",
    "output = tf.add_n(relus, name = \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the structure of the execution graph to a file\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Different scenarios of sharing variables between components ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. classical way of defining it outside the function and sending it as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# relu() function same as earlier\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]),1)\n",
    "        #intiating weights randomly\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name = \"weights\")\n",
    "        #bias term\n",
    "        b = tf.Variable(0.0, name = \"bias\")\n",
    "        # computing a linear function of inputs\n",
    "        z = tf.add(tf.matmul(X, w),b, name = \"z\")\n",
    "        return tf.maximum(z, threshold, name = \"max\")\n",
    "\n",
    "#creating the threshold variable\n",
    "threshold = tf.Variable(0.0, name = \"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_features), name = \"X\")\n",
    "relus = [ relu(X, threshold) for i in range(5) ]\n",
    "output = tf.add_n(relus, name = \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set the shared variable as an attribute of the function upon the first call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# relu() function same as earlier\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            #variable name is prefixed by the name scope\n",
    "            relu.threshold = tf.Variable(0.0, name = \"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]),1)\n",
    "        #intiating weights randomly\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name = \"weights\")\n",
    "        #bias term\n",
    "        b = tf.Variable(0.0, name = \"bias\")\n",
    "        # computing a linear function of inputs\n",
    "        z = tf.add(tf.matmul(X, w),b, name = \"z\")\n",
    "        return tf.maximum(z, relu.threshold, name = \"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_features), name = \"X\")\n",
    "relus = [ relu(X) for i in range(5) ]\n",
    "output = tf.add_n(relus, name = \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. use get_variable() to create the shared variable if it does not exist yet or reuse it if it already exists\n",
    "reset_graph()\n",
    "# this code will create a variable names relu/threshold using 0.0 as initial value\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\",shape = (),\n",
    "                               initializer = tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting reuse attribute to reuse a variable already created (in this case no need to specify shape or initializer)\n",
    "with tf.variable_scope(\"relu\", reuse = True):\n",
    "    threshold = tf.get_variable(\"threshold\")\n",
    "# an exception will be raised if it was not created using get_variable\n",
    "\n",
    "# setting reuse using reuse_variables() method within the score\n",
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# relu() function reusing the already created variable\n",
    "def relu(X):\n",
    "    # reusing variables with relu/ scope\n",
    "    with tf.variable_scope(\"relu\", reuse = True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]),1)\n",
    "        #intiating weights randomly\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name = \"weights\")\n",
    "        #bias term\n",
    "        b = tf.Variable(0.0, name = \"bias\")\n",
    "        # computing a linear function of inputs\n",
    "        z = tf.add(tf.matmul(X, w),b, name = \"z\")\n",
    "        return tf.maximum(z, threshold, name = \"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_features), name = \"X\")\n",
    "# initializing the threshold value\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer = tf.constant_initializer(0.0))\n",
    "relus = [ relu(X) for i in range(5) ]\n",
    "output = tf.add_n(relus, name = \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. use get_variable() to create the shared variable in the function itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    # initializing the variable within the relu()\n",
    "    threshold = tf.get_variable(\"threshold\", shape = (), initializer = tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]),1)\n",
    "    #intiating weights randomly\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name = \"weights\")\n",
    "    #bias term\n",
    "    b = tf.Variable(0.0, name = \"bias\")\n",
    "    # computing a linear function of inputs\n",
    "    z = tf.add(tf.matmul(X, w),b, name = \"z\")\n",
    "    return tf.maximum(z, threshold, name = \"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_features), name = \"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    # setting reuse = None on the first call, and reuse = True for the other calls\n",
    "    with tf.variable_scope(\"relu\", reuse = (relu_index >= 1 or None)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name = \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
