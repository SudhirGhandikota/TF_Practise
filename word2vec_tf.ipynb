{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Implementing tensorflow version of word2vec line by line, to understand the nitty-gritties #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions download the given file from the \n",
    "def maybe_download(filename, expected_bytes,dirname):\n",
    "    \"\"\" Download the file if not present. \"\"\"\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    local_filename = os.path.join(dirname, filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        # download the given file name to the path specified\n",
    "        local_filename, _ = urllib.request.urlretrieve(url+filename, local_filename)\n",
    "            \n",
    "    statinfo = os.stat(local_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify' + local_filename)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function reads the data in the given filename(.zip) into a list of string\n",
    "def read_data(filename):\n",
    "    \"\"\" Extract the first file enclosed in a zip file as a list of words \"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        # reading the first file from the zip archive\n",
    "        # tf.compat.as_str() returns the file context read (via read()) as a string\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes in the data (as a list of words) and retrievs the most frequent ones based on their frequency\n",
    "# data -> ids of all the words. If the word is not chosen then its id = 0\n",
    "# counts -> key: word, value -> frequency\n",
    "# reversed_dictionary -> key: id, value -> word (contains only the most frequent words)\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\" Process raw text data into a dataset\"\"\"\n",
    "    counts = [['UNK', -1]]\n",
    "    # retrieving the most common words based on their frequencies\n",
    "    counts.extend(collections.Counter(words).most_common(n_words-1))\n",
    "    dictionary = {} # stores word as key and its order based on frequency as value\n",
    "    for word, _ in counts:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    unk_cnt = 0\n",
    "    # this loop loops through the entire word list and assigns any word not selected earlier (as the most frequenct ones) as 'unknown'\n",
    "    for word in words:\n",
    "        index = dictionary.get(word,0) # if the word does not exist in the dictionary then index = 0\n",
    "        if index ==0: # i.e if the word is not part of the dictionary, assigned to unknown\n",
    "            unk_cnt += 1\n",
    "        data.append(index) # storing the indexes of selected words\n",
    "    counts[0][1] = unk_cnt\n",
    "    # reversed_dictionary: key -> id, value -> word\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, counts, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function generates a random batch of given size for the skip-gram model\n",
    "# skip_window -> window size (i.e. how many words to consider left and right )\n",
    "# num_skips -> how many times to reuse an input to generate a label \n",
    "# if num_skips = 2, it means one (word,context) will be used twice, with one being the target and the other being the context\n",
    "# i.e same combination of context words used num_skip times with the same target word as label\n",
    "def generate_batch(data,batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2*skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype = np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype = np.int32)\n",
    "    span = 2 * skip_window + 1 # [skip_window, target, skip_window]\n",
    "    # doubly ended queue to store number of time a word has been part of a batch. deque's support fast O(1) operations to append and pop\n",
    "    buffer = collections.deque(maxlen = span) \n",
    "    # resetting the data_index if it grows longer/bigger than the data size\n",
    "    if data_index + span > len(data): \n",
    "        data_index = 0\n",
    "    #append to the deque (initially first 3 words in the data would be added)\n",
    "    buffer.extend(data[data_index:data_index + span]) \n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_word_ids = [w for w in range(span) if w!= skip_window]\n",
    "        words_to_use = random.sample(context_word_ids, num_skips)\n",
    "        for j, context_word_id in enumerate(words_to_use):\n",
    "            batch[i*num_skips + j] = buffer[skip_window] # setting the skip_window\n",
    "            labels[i*num_skips+j,0] = buffer[context_word_id] #setting the context word as the label\n",
    "        # if end of the words reached, then starting over again\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        # adding the next word\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # back track a little bit to avoid skipping words in the end of the batch\n",
    "    data_index = (data_index + len(data) - span) & len(data)\n",
    "    print(\"Buffer: \",buffer)\n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions implements the vanilla version of word2vec\n",
    "# log_dir -> path to a log directory to save the tensorboard summaries\n",
    "def word2vec_basic(log_dir):\n",
    "    # create the directory for TensorBoard variables if there is not\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    # downloading file from http://mattmahoney.net/dc/\n",
    "    filename = maybe_download('text8.zip',31344016,'..')\n",
    "    \n",
    "    # reading the content first file in the zip archive as a list of words\n",
    "    data = read_data(filename)\n",
    "    print('Data Size:', len(data))\n",
    "    \n",
    "    vocab_size = 50000 # size of the vocabulary i.e most frequent words\n",
    "    # data -> list of ids ( intergers from 0 to vocabulary_size - 1). Original text where words replaced by their IDs\n",
    "    # counts -> map of words (strings) to their frequencies\n",
    "    # dictionary -> map of words (strings) to their ids(integer)\n",
    "    # reversed_dictionary -> map of ids (integer) to their words (string)\n",
    "    data, counts, dictionary, reversed_dictionary = build_dataset(data, vocab_size)\n",
    "    print('Most common words (including UNK): ',counts[:5])\n",
    "    print('Sample Data: ',data[:10], [reversed_dictionary[i] for i in data[:10]])\n",
    "    \n",
    "    batch, labels = generate_batch(data, batch_size = 16, num_skips = 2, skip_window = 1)\n",
    "    print(batch.shape, labels.shape)\n",
    "    for i in range(16):\n",
    "        print(batch[i], reversed_dictionary[batch[i]], '->', labels[i,0], reversed_dictionary[labels[i,0]])\n",
    "    \n",
    "    # initializing model parameters\n",
    "    batch_size = 128\n",
    "    embedding_size = 128\n",
    "    skip_window = 1 # number of words to count left and right\n",
    "    num_skips = 2 # how many times to reuse an input to generate a label\n",
    "    num_sampled = 64 # number of negative examples to sample -> negative sampling\n",
    "    \n",
    "    # Picking a random validation set to sample nearest neighbors. Used to display model accuracy\n",
    "    # the validation samples limited to words that have a low numeric ID, which are also the most frequent ones\n",
    "    # initiating the tensorflow execution graph\n",
    "    valid_size = 16 # random set of words to evaluate similarity on\n",
    "    valid_window = 100 # only pick dev samples in the head of the distribution i.e. the most frequent ones\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace = False)\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        # initiating input tensors\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            # IDs of target words, declared as integers\n",
    "            train_inputs = tf.placeholder(tf.int32, shape = [batch_size])\n",
    "            # IDs of context words i.e. labels, declared as integers\n",
    "            train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1])\n",
    "            # validation dataset which are randomly generated\n",
    "            validation_set = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data Size: 17005207\n",
      "Most common words (including UNK):  [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample Data:  [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "Buffer:  deque([59, 156, 128], maxlen=3)\n",
      "(16,) (16, 1)\n",
      "3081 originated -> 5234 anarchism\n",
      "3081 originated -> 12 as\n",
      "12 as -> 6 a\n",
      "12 as -> 3081 originated\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "2 of -> 195 term\n",
      "2 of -> 3134 abuse\n",
      "3134 abuse -> 2 of\n",
      "3134 abuse -> 46 first\n",
      "46 first -> 59 used\n",
      "46 first -> 3134 abuse\n",
      "59 used -> 46 first\n",
      "59 used -> 156 against\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-38e77a08669a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# implementing word2vec_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m# maintains a running index of the words traversed for generating the training batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mword2vec_basic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec_logs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-a64d011a7855>\u001b[0m in \u001b[0;36mword2vec_basic\u001b[1;34m(log_dir)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# IDs of target words, declared as integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mtrain_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;31m# IDs of context words i.e. labels, declared as integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# implementing word2vec_basic\n",
    "data_index = 0 # maintains a running index of the words traversed for generating the training batches\n",
    "word2vec_basic('word2vec_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = collections.deque(maxlen = 2) \n",
    "test.extend([1,2])\n",
    "print(test)\n",
    "test.append(3)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
